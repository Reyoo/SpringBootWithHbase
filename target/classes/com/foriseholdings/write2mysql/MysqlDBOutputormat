package com.foriseholdings.write2mysql;

import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.db.DBConfiguration;
import org.apache.hadoop.mapreduce.lib.db.DBOutputFormat;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;

import com.foriseholdings.commons.utils.PropertyUtil;
import com.foriseholdings.write2mysql.Mapper.DBOutputMapper1;
import com.foriseholdings.write2mysql.Reduce.DBReducer1;


public class MysqlDBOutputormat {

	private static String jdbcDri = PropertyUtil.getProperty("jdbcDri");
	private static String username = PropertyUtil.getProperty("username");
	private static String password = PropertyUtil.getProperty("password");
//	String inputHdfs = PropertyUtil.getProperty("hdfs");
	public static String inputHdfs = "/itemCF/20171228/step6_output/part-r-00000";
	private static String hdfs = "hdfs://192.168.92.215:9000";
	

	public static void main(String[] args) {
		int result = -1;
		result = new MysqlDBOutputormat().run();
		if (result == 1) {
			System.out.println("step1成功");
		} else if (result == -1) {
			System.out.println("step1失败");
		}

	}

	public int run() {
		try {
			// 读取配置文件
			Configuration conf = new Configuration();

			conf.set("fs.defaultFS", hdfs);
			DBConfiguration.configureDB(conf, "com.mysql.jdbc.Driver", jdbcDri, username, password);
			// 新建一个任务
			// Job job = new Job(conf, "DBOutputormatDemo");
			Job job = Job.getInstance(conf, "DBOutputormatDemo");
			// 设置主类
			job.setJarByClass(MysqlDBOutputormat.class);
			// Mapper
			job.setMapperClass(DBOutputMapper1.class);
			// Reducer
			job.setReducerClass(DBReducer1.class);
			
			// 输入路径
			// FileInputFormat.addInputPath(job, new Path(arg0[0]));
			FileSystem fs = FileSystem.get(conf);
			Path inputPath = new Path(hdfs+inputHdfs);
			if (fs.exists(inputPath)) {
				FileInputFormat.addInputPath(job, inputPath);
			}

			// mapper输出格式
			job.setOutputKeyClass(LongWritable.class);
			job.setOutputValueClass(Text.class);

			job.addArchiveToClassPath(new Path("hdfs://192.168.92.215:9000/lib/mysql/mysql-connector-java-5.1.38.jar"));
			// 输入格式，默认就是TextInputFormat
			// job.setInputFormatClass(TextInputFormat.class);
			// 输出格式
			job.setOutputFormatClass(DBOutputFormat.class);
//			 job.waitForCompletion(true);  
			// 输出到哪些表、字段
			DBOutputFormat.setOutput(job, "tb_recommended_sort", "user_id", "prods", "bus_code", "algorithm_type",
					"algorithm_date");
			return job.waitForCompletion(true) ? 0 : 1;
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();

		} catch (ClassNotFoundException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		} catch (InterruptedException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}

		return -1;
	}

}
